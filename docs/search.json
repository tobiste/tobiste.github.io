[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          university\n        \n         \n          number\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nuniversity\n\n\nnumber\n\n\n\n\n\n\nStructural Geology & Tectonics\n\n\nLakehead University\n\n\nGEOL-3310\n\n\n\n\nCase Studies\n\n\nLakehead University\n\n\nGEOL-4313\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "My developments are located on my GitHub page. They are free and open-source and I try to keep them up to date. Feel free to contribute, comment or open an issue!"
  },
  {
    "objectID": "software.html#r-packages",
    "href": "software.html#r-packages",
    "title": "Software",
    "section": "R packages",
    "text": "R packages\n\ntectonicr\n\n\ntectonicr is a free and open-source R package for modeling and analyzing the direction of the maximum horizontal stress (SHmax) based on the empirical link between the orientation of intraplate stress and the direction of the relative motion of neighboring plates (see Stephan et al., 2023). You can download and install it directly from the CRAN server using install.packages(\"tectonicr\") in R. A beta version is on github.\ndocumentation | CRAN | code | GUI\n\n\n\n\n\n\n\ngeoprofiler\ngeoprofiler is a free and open-source R package for creating SWATH profiles and data transects, i.e. geoscience parameters plotted against the distance along a user-defined profile line. Useful when one wants to plot the trend of values (e.g. geochemical compositions, ages, depth, magnitudes etc.) against the distance to a important structure. You can download and install it directly from the CRAN server using install.packages(\"geoprofiler\") in R. A beta version is on github.\ndocumentation | CRAN | code\n\n\nstructr (beta)\n\n\nStructural geology package for R, free and open-source. It provides functions to\n\nanalyze and visualize orientation data such as faults, folds, bedding planes, and lineations.\nanalyze and visualize the magnitudes of stress in the Mohr Circle.\nreconstruct the orientation of structures in oriented drill cores using the alpha, beta, and gamma angles\npaleostress analysis using slip inversion\nvorticity and strain analysis\nsimulation of deformation using deformation and velocity gradient tensors\neasy import: import your structural data directly from StraboSpot projects\n\ndocumentation | code\n\n\n\n\n\n\n\nlaftr\nCalculates the ages from LA-ICP-MS based fission track dating.\ndocumentation | code\n\n\neuler\nRotation of plates in terms of quaternions: code\n\n\neuler.reco\nFinds best Euler pole solution for a given structure: code"
  },
  {
    "objectID": "software.html#web-applications",
    "href": "software.html#web-applications",
    "title": "Software",
    "section": "Web applications",
    "text": "Web applications\n\ntectonicr\nGUI for tectonicr as browser App (no R installation required): tectonicr-app\n\n\nMohr Circle App\nPlotting Mohr Circles for students and teachers: GUI | code\n\n\napfu converter\nConvert Weight% to Molar weight (g/mol) or apfu (atoms per formula unit): GUI | code\n\n\nTheriak-Domino helpers\nWeb-application providing little helper functions to handle THERIAK DOMINO files:\n\nTherin Generator: Calculates the molar units given a bulk compositions (wt. %) and generates the THERIN input for Theriak-Domino\nGUZZLER table filter: Extracts minerals from the GUZZLER reaction table\n\nGUI | code"
  },
  {
    "objectID": "research/stress/index.html",
    "href": "research/stress/index.html",
    "title": "Analyzing stress and strain fields",
    "section": "",
    "text": "The spherical shape of the Earth curves straight orientations on the Earth’s surface. Thus, a statistical analysis of large tectonic fields, such as stress and strain, is challenging as long as there is no reference system. In this project, we analyse these fields with respect to the first-order source, plate boundary forces.\n\n\nAudio\nAI-generated1 audio summary\nSince these forces are at an angular relationship to the plate motion between two plate, the pole of rotation serves as an new reference system. Plate motion parameters are easy to determine and the new reference system also allows for testing and predicting stress fields. This is in particular useful in areas where stress and strain data are not available (Stephan & Enkelmann, 2025). And the method can be applied for paleo-stress fields if the relative plate motion from plate reconstructions is known.\nMy work in usable statistical tools for stress/strain orientations is nascent, but follows from my existing work in improving statistical communication in the fields of structural geology and plate motion reconstruction."
  },
  {
    "objectID": "research/stress/index.html#footnotes",
    "href": "research/stress/index.html#footnotes",
    "title": "Analyzing stress and strain fields",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ngenerated using Google NotebookLM↩︎"
  },
  {
    "objectID": "research/plate/index.html",
    "href": "research/plate/index.html",
    "title": "Plate motion",
    "section": "",
    "text": "Every motion on the earth surface can be described as a rotation around an axis piercing through the Earth’s center. Thus, also plate motion is mathematically described as the rotation of spherical polygons around rotational axes (so called Euler poles or Poles of Rotation). But how do we describe plate motion if these poles migrate as well?\n\n\nAudio\nAI-generated[^1] audio summary\n\n\n\n\n\n(Schaeben et al., 2023, fig. 4).\n\n\nIn this interdisciplinary project, we use the quaternion formulation of rotations to see what happens to the three axes in a three plate scenario when we change our reference system (usually one plate). We show that in such a scenario, two rotational axis can stay fixed while the third one is constantly changing its location and rotation speed. Thus, a change of plate motion is not only a consequence of complex geodynamic changes - it can be due to a much easier cause: spherical geometry. This does not only challenge our way of reconstructing past and current plate motion, it does also has physical consequences on the deformation of plates.\n\n\n\n(Schaeben et al., 2023, fig. 5).\n\n\n\n\n\n\nReferences\n\nSchaeben, H., Kroner, U., & Stephan, T. (2022). Euler Poles of Tectonic Plates. In B. S. Daza Sagar, Q. Cheng, J. McKinley, & F. Agterberg (Eds.), Encyclopedia of mathematical geosciences. Encyclopedia of earth sciences series (pp. 1–6). Springer Nature Switzerland AG 2021. https://doi.org/10.1007/978-3-030-26050-7_435-2\n\n\nSchaeben, H., Kroner, U., & Stephan, T. (2023). Mathematical fundamentals of spherical kinematics of plate tectonics in terms of quaternions. Mathematical Methods in the Applied Sciences, 47(6), 4469–4496. https://doi.org/10.1002/mma.9823"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Web of Science\n\nStephan, T., Phillips, N., Tiitto, H., Perez, A., Nwakanma, M., and Hollings, P. (2025) “Going with the flow - Changes of Vorticity Controls Gold Enrichment in Archean Shear Zones (Shebandowan Greenstone Belt, Superior Province, Canada)”. Journal of Structural Geology. doi: 10.1016/j.jsg.2025.105542 | citation | \nStephan, T., and Enkelmann, E. (2025) “All Aligned on the Western Front of North America? Analyzing the Stress Field in the Northern Cordillera”. Tectonics. doi: 10.1029/2025TC009014 | citation | \nPadgett, J., Enkelmann, E., Kellett, D., Moynihan, D., and Stephan, T. (2025) “Cenozoic faulting in the Upper Hyland River Valley, Southeastern Yukon: A thermochronological perspective”. Canadian Journal of Earth Sciences 62(5): 1002-1022. doi: 10.1139/cjes-2024-0147 | citation | \nSchaeben, H., Kroner, U., and Stephan, T. (2024): “Mathematical Fundamentals of Spherical Kinematics of Plate Tectonics in Terms of Quaternions”. Mathematical Models and Methods in Applied Sciences 47(6): 4469-4496. doi: 10.1002/mma.9823 | citation | \nStephan, T., Enkelmann, E., and Kroner, U. (2023): “Analyzing the horizontal orientation of the crustal stress adjacent to plate boundaries”. Scientific Reports 13:15590. doi: 10.1038/s41598-023-42433-2 | citation | \nJáróka, T., Pfänder, J. A., Seifert, T., Hauff, F., Sperner, B., Staude, S., Stephan, T., and Schulz, B. (2023): “Age and petrogenesis of Ni-Cu-(PGE) sulfide-bearing gabbroic intrusions in the Lausitz Block, northern Bohemian Massif (Germany/Czech Republic)”. Lithos 444-445:107090. doi: 10.1016/j.lithos.2023.107090 | citation\nKroner, U., Romer, R. L., and Stephan, T. (2023): “Die Rekonstruktion von relativen Plattenbewegungen aus dem paläozoischen Deformationsmuster der kontinentalen Kruste”. Zeitschrift der Deutschen Gesellschaft für Geowissenschaften (J. Appl. Reg. Geol.). doi: 10.1127/zdgg/2023/0365 | citation\nKöhler, S., Duschl, F., Fazlikhani, H., Koehn, D., Stephan, T., and Stollhofen, H. (2022): “Reconstruction of cyclic Mesozoic-Cenozoic stress development in SE Germany using fault-slip and stylolite inversion”. Geological Magazine 159 (11–12): 2323-2345. doi: 10.1017/S0016756822000656 | citation | \nKroner, U., Stephan, T., and Romer, R. L. (2022): “Paleozoic orogenies and relative plate motions at the sutures of the Iapetus-Rheic Ocean”. In Y. D. Kuiper, J. B. Murphy, R. D. Nance, R. A. Strachan, and M. D. Thompson (Eds.), New Developments in the Appalachian-Caledonian-Variscan Orogen. Geological Society of America. doi: 10.1130/2021.2554(01) | citation\nSchaeben, H., Kroner, U., and Stephan, T. (2021): “Euler Poles of Tectonic Plates”. In B. S. Daza Sagar, Q. Cheng, J. McKinley, and F. Agterberg (Eds.), Encyclopedia of Mathematical Geosciences. Encyclopedia of Earth Sciences Series. doi: 10.1007/978-3-030-26050-7_435-1 | citation\nCaracciolo, L., Ravidà, D. C. G., Chew, D., Janßen, M., Lünsdorf, N. K., Heins, W. A., Stephan, T., and Stollhofen, H. (2021): “Reconstructing environmental signals across the Permian-Triassic boundary in the SE Germanic Basin: A Quantitative Provenance Analysis (QPA) approach”. Global and Planetary Change, 206:103631. doi: 10.1016/j.gloplacha.2021.103631 | citation\nKroner, U., Stephan, T., Romer, R. L., and Roscher, M. (2020): “Paleozoic plate kinematics during the Pannotia–Pangaea supercontinent cycle”. Geological Society, London, Special Publications 503, SP503-2020-15. doi: 10.1144/SP503-2020-15 | citation\nStephan, T., Kroner, U., Romer, R. L., and Rösel, D. (2019): “From a bipartite Gondwana shelf to the arcuate Variscan belt: The Early Paleozoic evolution of northern Peri-Gondwana”. Earth-Science Reviews 192: 491–512. doi: 10.1016/j.earscirev.2019.03.012 | citation\nHeinicke, J., Stephan, T., Alexandrakis, C., Buske, S., and Gaupp, R. (2019): “Alteration as possible cause for transition from brittle failure to aseismic slip: the case of the NW-Bohemia / Vogtland earthquake swarm region”. Journal of Geodynamics 124: 79–92. doi: 10.1016/j.jog.2019.01.010 | citation\nStephan, T., Kroner, U., and Romer, R. L. (2018): “The pre-orogenic detrital zircon record of the Peri-Gondwanan crust”. Geological Magazine 156(2): 281–307. doi: 10.1017/s0016756818000031 | citation\nStephan, T., Kroner, U., Hahn, T., Hallas, P., and Heuse, T. (2016): “Fold / cleavage relationships as indicator for late Variscan sinistral transpression at the Rheno-Hercynian – Saxo-Thuringian boundary zone, Central European Variscides”. Tectonophysics 681: 250–262. doi: 10.1016/j.tecto.2016.03.005 | citation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tobias Stephan",
    "section": "",
    "text": "I am an Assistant Professor at the Department of Geology at Lakehead University in Thunder Bay (Canada). I received a PhD in geology from Technische Universität Bergakademie Freiberg (Germany) in 2019.\nMy main research interest is the relationship between the deformation of rocks and the plate tectonic motion. To analyze these patterns from the micro-scale (thin sections) to large scale (tectonic plates), I use techniques from structural geology, metamorphic petrology, crystallographic preferred orientation, geophysics, geochemistry, geo-thermochronology, and provenance analysis. My research is based on field work, lab work, and computer modelling.\nI teach courses on structural geology, tectonics, metamorphic petrology, and data science for geology."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick-Guide to THERIAK-DOMINO\n\n\n\n\n\n\nMetamorphic petrology\n\n\nTHERIAK-DOMINO\n\n\n\nA simple step-by-step guide to use generate metamorphic phase equilibria diagrams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics and composition data\n\n\n\n\n\n\nGeochemistry\n\n\nData\n\n\nR\n\n\n\nThe blog describes how compositional data (geochemical compositons, concentrations, etc.) are treated to calculate summary statistics\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/compositions/index.html",
    "href": "blog/compositions/index.html",
    "title": "Statistics and composition data",
    "section": "",
    "text": "Show the required packages for this tutorial\nlibrary(fitdistrplus)\nlibrary(compositions)\nlibrary(Ternary)\nlibrary(RColorBrewer)\nlibrary(viridis)\nGeochemical data tables look like this:\ndata(Aar, package = \"compositions\")\nAar &lt;- Aar[c(1, 3:13)]\nhead(Aar)\n\n# A tibble: 6 × 12\n  Sample  SiO2  TiO2 Al2O3   MnO   MgO   CaO  Na2O   K2O  P2O5 Fe2O3t SumOxides\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 RT1-1   73.4  0.22  14.2 0.037  0.48  1.04  4.33  4.18  0.06   1.74      99.7\n2 RT1-1   73.9  0.26  14   0.038  0.51  1.14  4.32  3.95  0.07   1.72      99.9\n3 RT1-1   73.8  0.3   13.8 0.041  0.55  1.15  4.31  3.82  0.07   1.82      99.8\n4 RT1-1   78.1  0.32  11.4 0.043  0.57  1.02  3.41  3.16  0.06   1.81      99.9\n5 RT1-1   78.8  0.39  10.3 0.061  0.86  1.07  2.81  2.92  0.07   2.54      99.8\n6 RT1-1   74.4  0.54  11.9 0.073  0.94  2.22  2.95  3.2   0.29   2.96      99.5\nThey show the concentrations of certain oxides and elements. Geologists would then like to know if some of these concentrations correlate, if the samples are clustered, or simply how these concentrations are distributed. To find this out, we often explore them visually by plotting them, and sometimes perform statistical analyses. What we often forget is that every statistical analysis makes very specific assumptions about the nature of the underlying data."
  },
  {
    "objectID": "blog/compositions/index.html#why-are-most-people-doing-it-wrong",
    "href": "blog/compositions/index.html#why-are-most-people-doing-it-wrong",
    "title": "Statistics and composition data",
    "section": "Why are most people doing it wrong?",
    "text": "Why are most people doing it wrong?\nThe geochemical composition of rocks is commonly expressed as a weight percentage (wt%) or parts per million (ppm). Thus, these proportions are typically vectors of positive (or non-negative) numbers, where the sum is often a constant, such as 100%. However, they often do not actually add up to 100% due to data uncertainties, the detection limits of low-concentration elements or loss of ignition during laboratory analysis. This means the numbers cannot be smaller than 0 or exceed the constant (100%). In other words, these number vectors only form a subset of the real number system. In a ternary system, a composition cannot lie outside a ternary diagram.\nWe can plot the histograms of the data for each oxide and add a vertical line to indicate the location of the mean.\n\nHere I loop through all the columns using the lapply() function. The invisible() function at the end of the command chain makes sure that the output is the plot only.\n\n\n# Convert percantages to fractions so that numbers range between 0 and 1\nAar_oxides &lt;- Aar[, 2:11]/100\np &lt;- ncol(Aar_oxides)\nn &lt;- nrow(Aar_oxides)\n\n# Arrange the histogram in a 3 (rows) times 4 (columns) grid, and then make the \n# plot a bit more thigh by specifying the margins\npar(mfrow = c(3, 4), mar = c(4, 4, 1, 2.1))\n\nlapply(seq_len(p), function(i) {\n  oxide &lt;- as.numeric(as.matrix(Aar_oxides)[, i])\n  hist(oxide, main = NULL, xlab = colnames(Aar_oxides)[i], border = \"white\")\n  abline(v = mean(oxide), col = \"#B63679FF\", lwd = 2)\n}) |&gt;\n  invisible()\n\n\n\n\n\n\n\n\nThe fact that these numbers only occur within an interval has significant implications for the description of distributions. Firstly, we cannot assume that compositions are drawn from a Normal distribution. This means that our classical statistical parameters (mean, standard deviation, etc.) are somewhat meaningless when it comes to describing how the compositions are distributed. Standard stats may give misleading results, especially when values cluster near 0 or 1, or the distribution is skewed. For example, a mean near \\(0.95\\) with standard deviation \\(\\sigma^2 = 0.10\\) implies impossible values above 1 as the standard deviation does not respect the bounded nature of data and rarely reflects real uncertainty for skewed data. Since cluster analysis, regression and density estimation all rely on these parameters, most statistical analysis and visual exploration tools (e.g. box plots) will carry the same flaw: they do not represent the true distribution of your data!\n\n\nShow the code\nstats &lt;- sapply(\n  seq_len(p),\n  function(i) {\n    oxide &lt;- as.numeric(as.matrix(Aar_oxides)[, i])\n    c(summary(oxide), \"sd\" = sd(oxide))\n  }\n) |&gt;\n  t()\nrownames(stats) &lt;- colnames(Aar_oxides)\nprint(stats)\n\n\n          Min. 1st Qu.  Median         Mean  3rd Qu.    Max.           sd\nSiO2   0.49490 0.64515 0.72790 0.7080816092 0.771500 0.83690 0.0822708184\nTiO2   0.00100 0.00220 0.00300 0.0041241379 0.006000 0.01030 0.0025837581\nAl2O3  0.08470 0.11710 0.14490 0.1442804598 0.162900 0.20290 0.0314319525\nMnO    0.00018 0.00027 0.00038 0.0005514943 0.000655 0.00219 0.0004296227\nMgO    0.00160 0.00360 0.00520 0.0105678161 0.012250 0.05660 0.0117803196\nCaO    0.00430 0.00930 0.01140 0.0136425287 0.017100 0.02960 0.0061219892\nNa2O   0.01980 0.03330 0.04200 0.0408091954 0.048200 0.06330 0.0094424651\nK2O    0.01880 0.03030 0.03780 0.0377160920 0.044050 0.06200 0.0100624084\nP2O5   0.00030 0.00050 0.00080 0.0012482759 0.001850 0.00350 0.0009475626\nFe2O3t 0.00670 0.01120 0.01630 0.0249356322 0.030000 0.09360 0.0205773751\n\n\nOne simple solution is to calculate the statistics on the logarithmic scale. This will stretch all numbers close to 0 and 1 out to negative and positive infinity, respectively. This makes “normally” distributed data approach a symmetric, hence, Gaussian distribution in log space, known as the log-normal distribution, allowing us to use classic statistical estimators.\nA calculated mean in log space needs to transformed back into real space using the exp() function. This is essentially the geometric mean: \\[ \\text{geometric mean}(x) = \\text{exp}\\Bigl(\\frac{\\sum_{i=1}^{n} \\ln{x_i}}{n}\\Bigr)  \\]\n\n\nShow the code for the geometric mean in R\ngeomean &lt;- function(x, ...){\n  exp(mean(log(x), ...))\n}\n\n\nThe histograms for the log-transformed data shows that the distributions look symmetric about a mean:\n\n\nShow the code\npar(mfrow = c(3, 4), mar = c(4, 4, 1, 2.1))\nlapply(\n  seq_len(p),\n  function(i) {\n    oxide &lt;- as.numeric(as.matrix(Aar_oxides)[, i])\n    log_oxide &lt;- log(oxide)\n    hist(log_oxide,\n      main = NULL,\n      xlab = paste0(\"log(\", colnames(Aar_oxides)[i], \")\"),\n      border = \"white\"\n    )\n    abline(v = geomean(oxide), col = \"#B63679FF\", lwd = 2)\n  }\n) |&gt;\n  invisible()\n\n\n\n\n\n\n\n\n\nSome zero-value issues can be avoided using the Logit transformation: \\[ \\text{logit}(x) = \\log{\\frac{x}{1-x}} \\]\n\n\nShow the code for the logit-transformation in R\nlogit &lt;- function(x) log(x/(1-x))\n\nlogit_mean &lt;- function(x, ...) {\n  mu  &lt;- mean(logit(x), ...)\n  plogis(mu)\n}\n\n\nThe only problems with these log-based transformations is that \\(\\log(0) = \\infty\\) and \\(\\log(1) = 0\\) and \\(\\text{logit}(0) = -\\infty\\) and \\(\\text{logit}(1) = \\infty\\), which creates problems when the values are near 0 and 1.\nSome concentration data are measured as 0 (not detected) or 1 (pure component). In these cases, the log-transformation will create infinity values. Of course, we could replace such values by some adjustments, e.g. by assigning a constant to the below detection limits (BD), e.g. BD/2, or using some imputation algorithms. However any such adjustment will ultimately add bias. Moreover, the log-transformations are highly non-linear near 0 and 1. Small changes in small concentrations get magnified, and hence their distances will be distorted. This affects distance based methods to estimate the similarity among the data (e.g. clustering and regression algorithms)."
  },
  {
    "objectID": "blog/compositions/index.html#what-is-the-more-correct-way",
    "href": "blog/compositions/index.html#what-is-the-more-correct-way",
    "title": "Statistics and composition data",
    "section": "What is the more correct way?",
    "text": "What is the more correct way?\nThe good news is that mathematicians have found ways to handle distributions for such unconventional number systems, without having to transform our data. For numbers defined on an interval, we can use the Beta distribution. While a Normal distribution is described by a mean (\\(\\mu\\)) and a standard deviation (\\(\\sigma^2\\)), a Beta distribution is described by two shape parameters (\\(a\\) and \\(b\\), which are both non-negative real numbers).\nNotation:\n\nNormal distribution: \\(N(\\mu, \\sigma^2)\\)\nBeta distribution: \\(B(a, b)\\)\n\nHere is how they look like and how the parameters change the overall shape of the density functions.\n\n\nShow the code\ncols &lt;- magma(3, end = .9)\n# Generate a random sample of a Beta-distribution\nx1 &lt;- rbeta(1000, 1, 1)\nx2 &lt;- rbeta(1000, 1, 3)\nx3 &lt;- rbeta(1000, 3, 1)\nx4 &lt;- rbeta(1000, .5, 1)\nx5 &lt;- rbeta(1000, 1, .5)\n\npar(mfrow = c(1, 2))\nplot(density(x1), col = cols[1], ylim = c(0, 2.6), main = 'Probability density')\nlines(density(x2), col = cols[2])\nlines(density(x3), col = cols[3])\n\nplot(ecdf(x1), col = cols[1], main = 'Cumulative density')\nlines(ecdf(x2), col = cols[2])\nlines(ecdf(x3), col = cols[3])\nlegend(\"topleft\", col = cols, lty = 1, legend = c(\"a=b=1\", \"a=1, b=3\", \"a=3, b=1\"))\n\n\n\n\n\n\n\n\n\nSee how they resemble the distribution of our geochemical data! Analogous to the normal distribution, the goal now is to find these parameters that best describe the distribution of our data."
  },
  {
    "objectID": "blog/compositions/index.html#how-to-properly-do-the-descriptive-inference",
    "href": "blog/compositions/index.html#how-to-properly-do-the-descriptive-inference",
    "title": "Statistics and composition data",
    "section": "How to properly do the descriptive inference?",
    "text": "How to properly do the descriptive inference?\n\nOpen your R editor (e.g. RStudio)\nImport your geochemical data\nLoad the following packages\nand load these functions for calculating summary statistics for the Beta distribution:\n\n\n\nShow the code for the underlying functions for the summary statistics of a Beta dsitributions\n# Fit of Beta distribution to data\nfitbeta &lt;- function(x, range = c(0, 1), na.rm = FALSE, method = \"mle\", ...) {\n  if (is.null(method)) {\n    xm &lt;- (mean(x, na.rm = na.rm) - range[1]) / (range[2] - range[1])\n    xv &lt;- var(x, na.rm = na.rm) / (range[2] - range[1])^2\n    temp1 &lt;- xm * (1 - xm)\n\n    if (xv &lt; temp1) {\n      temp2 &lt;- (temp1 / xv - 1)\n      alpha &lt;- xm * temp2\n      beta &lt;- (1 - xm) * temp2\n    } else {\n      alpha &lt;- NA\n      beta &lt;- NA\n    }\n    c(shape1 = alpha, shape2 = beta)\n  } else {\n    suppressWarnings(\n      res &lt;- fitdistrplus::fitdist(x, \"beta\", method, start = list(shape1=1, shape2=1), lower = 0, ...)\n    )\n    res$estimate\n  }\n}\n\n# Summary statistics of a Beta distribution\nbeta_mean &lt;- function(x, shape1 = NULL, shape2 = NULL) {\n  if (is.null(shape1) | is.null(shape2)) {\n    ab &lt;- fitbeta(x) |&gt; unname()\n    shape1 &lt;- ab[1]\n    shape2 &lt;- ab[2]\n  }\n  shape1 / (shape1 + shape2)\n}\n\nbeta_var &lt;- function(x, shape1 = NULL, shape2 = NULL) {\n  if (is.null(shape1) | is.null(shape2)) {\n    ab &lt;- fitbeta(x) |&gt; unname()\n    shape1 &lt;- ab[1]\n    shape2 &lt;- ab[2]\n  }\n\n  shape1 * shape2 / (\n    (shape1 + shape2)^2 * (shape1 + shape2 + 1)\n  )\n}\n\nbeta_sd &lt;- function(x, shape1 = NULL, shape2 = NULL) sqrt(beta_var(x, shape1, shape2))\n\nbeta_mode &lt;- function(x, shape1 = NULL, shape2 = NULL) {\n  if (is.null(shape1) | is.null(shape2)) {\n    ab &lt;- fitbeta(x) |&gt; unname()\n    shape1 &lt;- ab[1]\n    shape2 &lt;- ab[2]\n  }\n  if (shape1 &gt;= 1 & shape2 &gt;= 1) {\n    (shape1 - 1) / (shape1 + shape2 - 2)\n  } else if (shape1 &lt; 1 & shape2 &gt; 1) {\n    0\n  } else if (shape2 &lt; 1 & shape1 &gt; 1) {\n    1\n  } else {\n    NA\n  }\n}\n\nbeta_quantile &lt;- function(x, shape1 = NULL, shape2 = NULL, probs = seq(0, 1, 0.25)) {\n  if (is.null(shape1) | is.null(shape2)) {\n    ab &lt;- fitbeta(x) |&gt; unname()\n    shape1 &lt;- ab[1]\n    shape2 &lt;- ab[2]\n  }\n  sapply(probs, qbeta, shape1 = shape1, shape2 = shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE) |&gt;\n    setNames(nm = as.character(probs))\n}\n\nbeta_median &lt;- function(x, shape1 = NULL, shape2 = NULL) {\n  beta_quantile(x, shape1, shape2, probs = 0.5) |&gt; unname()\n}\n\nbeta_ci &lt;- function(x, shape1 = NULL, shape2 = NULL, conf = .95){\n  if (is.null(shape1) | is.null(shape2)) {\n    ab &lt;- fitbeta(x) |&gt; unname()\n    shape1 &lt;- ab[1]\n    shape2 &lt;- ab[2]\n  }\n  \n  alpha &lt;- 1 - conf\n  ci_param &lt;- qbeta(c(alpha/2, 1 - alpha/2), shape1 = shape1, shape2 = shape2)\n}\n\nbeta_summary &lt;- function(x, ...) {\n  ab &lt;- fitbeta(x, ...) |&gt; unname()\n  c(\n    Min. = min(element),\n    Mean = beta_mean(NULL, ab[1], ab[2]),\n    Median = beta_median(NULL, ab[1], ab[2]),\n    \"1st Qu.\" = beta_quantile(NULL, ab[1], ab[2], 0.25),\n    \"3rd Qu.\" = beta_quantile(NULL, ab[1], ab[2], 0.75),\n    Max. = max(element)\n  )\n}\n\n\nLet’s create a random sample of a Beta function and calculate some statistics:\n\nelement &lt;- rbeta(1000, .3, 5)\n\n\nplot(density(element))\n\n\n\n\n\n\n\n\n\nfitbeta(element)\n\n   shape1    shape2 \n0.3119077 4.8944413 \n\n\nThe mean and the standard deviation of the Beta-distributed sample is\n\nbeta_mean(element)\n\n[1] 0.0599091\n\nbeta_sd(element)\n\n[1] 0.09526058\n\n\nOr use the beta_summary() function to get them all (mean, median, and quantiles) in one call:\n\nbeta_summary(element)\n\n        Min.         Mean       Median 1st Qu..0.25 3rd Qu..0.75         Max. \n1.244227e-12 5.990910e-02 1.758985e-02 1.819481e-03 7.638578e-02 7.461490e-01 \n\n\nNow compare the differences of the statistical estimators for the different distributions. Here are the different estimates for the mean:\n\n\nShow the code\n# Mean\nmeans &lt;- setNames(\n  c(\n    mean(element),\n    geomean(element),\n    logit_mean(element),\n    beta_mean(element)\n  ), c(\"Normal\", \"Log-Normal\", \"Logit-Normal\", \"Beta\")\n)\nprint(means)\n\n\n      Normal   Log-Normal Logit-Normal         Beta \n 0.060347669  0.007354321  0.007812620  0.059909103 \n\n\nHere are the different estimates for the standard deviation:\n\n\nShow the code\n# Standard deviation\nsetNames(\n  c(\n    sd(element),\n    sd(log(element)),\n    sd(logit(element)),\n    beta_sd(element)\n  ), c(\"Normal\", \"Log-Normal\", \"Logit-Normal\", \"Beta\")\n)\n\n\n      Normal   Log-Normal Logit-Normal         Beta \n  0.09248177   3.52738372   3.58811171   0.09526058 \n\n\nYou can see that the Log-Normal distribution gives very different mean and standard deviation values. The means all look similar, yet they are not identical. The standard deviation do look more different. The log- and logit transformed values are hard to interpret, and a back-transformation into real space is not straight-forward.\nSo what’s the problem? A normal distributed data is described by \\(\\mu \\pm \\sigma^2\\), meaning the data is symmetric about the mean. However, since our data is clamped between 0 and 1, strongly skewed values can lead to negative predicted values or values that are larger than 1. But we know this is not allowed! Hence, this estimators cannot describe nor predict the data. This problem becomes more significant the more skewed and tailed our data is.\nOn the other hand, the Beta standard deviation indicates how concentrated or dispersed the distribution is around the mean, considering the Beta’s shape (smaller SD means data are more concentrated or peaked near the mean, while larger SD means more dispersed, flatter, more skewed data). It is only a summary measure, not a bound! So for heavily skewed Beta (a ≪ b or a ≫ b), it is better to report quantiles or confidence intervals instead."
  },
  {
    "objectID": "blog/compositions/index.html#inference",
    "href": "blog/compositions/index.html#inference",
    "title": "Statistics and composition data",
    "section": "Inference",
    "text": "Inference\nThe central limit theory states that the distribution of sample means will be approximately normal if the sample size is large, regardless of the original population’s distribution. This theorem provides an alternative statistical inference technique, called bootstrapping, which does not require assumptions on the underlying population distribution. Hence it provides non-parametric estimate of a statistic.\nBootstrapping uses re-sampling of the data with replacement, to estimate the sampling distribution of a statistic, such as the Beta mean.\nHere we calculate oxides’ Beta mean for 10,000 bootstrap samples.   \n\n# number of bootstrap samples:\nB &lt;- 10000\n\noxides_beta_boot &lt;- sapply(1:B, function(i) {\n  idx &lt;- sample.int(n, n, replace = TRUE)\n  apply(Aar_oxides[idx, ], 2, beta_mean)\n})\n\nWe can inspect the distribution of these bootstrapped samples means:\n\n\nShow the code\npar(mfrow = c(3, 4), mar = c(4, 4, 1, 2.2))\nlapply(seq_len(nrow(oxides_beta_boot)), function(i) {\n  hist(t(oxides_beta_boot[i, ]), border = \"white\", xlab = rownames(oxides_beta_boot)[i], main = NA)\n}) |&gt;\n  invisible()\n\n\n\n\n\n\n\n\n\nThe 2.5% and 97.5% quantile for each of these bootstrap samples are an non-parametric estimate for the 95% confidence interval of our Beta mean:\n\noxides_beta_CI &lt;- apply(oxides_beta_boot, 1, quantile, probs = c(.025, .975))\nprint(oxides_beta_CI)\n\n           SiO2        TiO2     Al2O3          MnO         MgO        CaO\n2.5%  0.6903563 0.003607587 0.1377247 0.0004686771 0.008321188 0.01241389\n97.5% 0.7241500 0.004676284 0.1508254 0.0006451746 0.013165865 0.01495236\n            Na2O        K2O        P2O5     Fe2O3t\n2.5%  0.03881916 0.03564495 0.001058528 0.02097235\n97.5% 0.04277645 0.03985792 0.001449542 0.02946774\n\n\nNow add the confidence interval of the Beta mean as an vertical rectangle to our histograms:\n\n\nShow the code\npar(mfrow = c(3, 4), mar = c(4, 4, 1, 2.1))\nlapply(seq_len(p), function(i) {\n  oxide &lt;- Aar_oxides[, i]\n  hist(oxide, main = NULL, xlab = colnames(Aar_oxides)[i], border = \"white\")\n  rect(oxides_beta_CI[1, i], -10, oxides_beta_CI[2, i], 100, col = adjustcolor(\"#B63679FF\", alpha.f = 0.2), border = NA)\n  abline(v = beta_mean(oxide), col = \"#B63679FF\", lwd = 2)\n}) |&gt;\n  invisible()"
  },
  {
    "objectID": "blog/compositions/index.html#which-distrbution-model-describes-my-data-the-best",
    "href": "blog/compositions/index.html#which-distrbution-model-describes-my-data-the-best",
    "title": "Statistics and composition data",
    "section": "Which distrbution model describes my data the best?",
    "text": "Which distrbution model describes my data the best?\nIn case you’re still not convinced, the {fitdistrplus} package offers some visual and statistical parameters to compare the goodness of fit of different distributions:\n\n\nShow the code\ntest_dat &lt;- Aar_oxides[, \"SiO2\"]\nfit_n &lt;- fitdist(test_dat, \"norm\")\nfit_ln &lt;- fitdist(test_dat, \"lnorm\")\nfit_lg &lt;- fitdist(test_dat, \"logis\")\nfit_b &lt;- fitdist(test_dat, \"beta\")\n\ndistlist &lt;- list(fit_n, fit_ln, fit_lg, fit_b)\nnames(distlist) &lt;- c(\"normal\", \"lognormal\", \"logis\", \"beta\")\n\ngofstat(distlist, fitnames = names(distlist))\n\n\nGoodness-of-fit statistics\n                                normal lognormal     logis      beta\nKolmogorov-Smirnov statistic 0.1694723 0.1934172 0.1261877 0.1499985\nCramer-von Mises statistic   0.3500655 0.4878448 0.2313566 0.2487387\nAnderson-Darling statistic   1.8526410 2.6310094 1.5701612 1.2804029\n\nGoodness-of-fit criteria\n                                  normal lognormal     logis      beta\nAkaike's Information Criterion -184.7170 -175.7346 -183.5007 -191.5377\nBayesian Information Criterion -179.7852 -170.8028 -178.5689 -186.6059\n\n\nHere we check which distribution better fits the SiO2 concentrations, and the lowest Akaike’s and Bayesian Information criteria, as well as the low statistics all indicate that the Beta distribution best explain the concentrations. We can also have a visual representation of the data fits:\n\n\nShow the code\npar(mfrow = c(2, 2))\ncdfcomp(distlist, legendtext = names(distlist))\ndenscomp(distlist, legendtext = names(distlist))\nqqcomp(distlist, legendtext = names(distlist))\nppcomp(distlist, legendtext = names(distlist))"
  },
  {
    "objectID": "blog/compositions/index.html#ternary-systems",
    "href": "blog/compositions/index.html#ternary-systems",
    "title": "Statistics and composition data",
    "section": "Ternary systems",
    "text": "Ternary systems\nSo far we only looked at 1-dimension data. But how do we deal with more dimension, i.e., more than just one element (or oxide) concentration?\nThe Beta distribution is the 1-dimensional equivalent of a the Dirichlet distribution, which just has some more parameters to describe the multidimensional shapes. Otherwise nothing changes: estimate the shape parameters of the distribution, and then calculate the summary statistic.\n\nAar_ACF &lt;- cbind(\n  Aar$Al2O3 + Aar$Fe2O3t - (Aar$Na2O + Aar$K2O),\n  Aar$CaO + 10 / 3 * Aar$P2O5,\n  Aar$Fe2O3t + Aar$MgO + Aar$MnO\n)\n\nmy_categories &lt;- factor(Aar$Sample)\nnum_unique_categories &lt;- nlevels(my_categories)\npalette_colors &lt;- RColorBrewer::brewer.pal(num_unique_categories, \"Set1\")\nassigned_colors &lt;- palette_colors[as.numeric(my_categories)]\n\nTernary::TernaryPlot(\n  main = \"ACF\",\n  alab = expression(\"Al\"[2] * \"O\"[3] + \"Fe\"[2] * \"O\"[3] - \"(Na\"[2] * \"O\" + \"K\"[2] * \"O)\"),\n  clab = expression(\"CaO\" + 10 / 3 * \"(P\"[2] * \"O\"[5] * \")\"),\n  blab = expression(\"FeO\" + \"MgO\" + \"MnO\"),\n  lab.font = 4,\n  axis.font = 1,\n  clockwise = FALSE, grid.lines = 5, grid.minor.lines = 1\n)\nTernary::TernaryPoints(Aar_ACF[, 1:3], pch = 16, cex = .8, col = assigned_colors)\n\n\n\n\n\n\n\n\nMy code below defines a Dirichlet kernel that will be used to calculate densities in a ternary diagram. In case your are interested, the functions create a grid of equally spaced points within the ternary diagram, then loops through every grid point and calculate the Dirichlet distribution for the close neighbors, and finally adds all the distributions.\n\n\nShow the code for the underlying functions\n# Calculate the distance between two compositions x and y in a ternary system.\n# x and y are both three-element vectors.\ntern_distance &lt;- function(x, y) {\n  sqrt(0.5 * ((x[1] - y[1])^2 + (x[2] - y[2])^2 + (x[3] - y[3])^2))\n  \n}\n\n# Dirichlet KDE for ternary diagrams. \n# \n# h is the bandwidth (usually a value between 0 and 1), \n# and n is the grid resolution (amount of points per axis)\nkde_Dirichlet &lt;- function(x, h, n = 100, ...) {\n  vals &lt;- seq(0, 1, length.out = n)\n  kdegrid &lt;- expand.grid(x = vals, y = vals, z = vals) |&gt;\n    subset(x + y + z == 1)\n\n  x[x &lt;= 0] &lt;- .Machine$double.eps^0.5\n  x &lt;- x / rowSums(x)\n\n  dummy &lt;- function(a, b, c) {\n    a + b + c\n  }\n  dumy_grid &lt;- Ternary::TernaryPointValues(dummy, resolution = n)\n\n  nx &lt;- nrow(x)\n\n  values &lt;- lapply(seq_len(nrow(kdegrid)), function(i) {\n    kcenter &lt;- as.numeric(kdegrid[i, ])\n\n    xd &lt;- sapply(seq_len(nx), function(k) {\n      tern_distance(x[k, ], kcenter)\n    })\n    idx &lt;- xd &lt;= h\n\n    if (sum(idx) &gt; 2) {\n      dirichlet_params &lt;- compositions::acomp(x[idx, ]) |&gt; \n        compositions::fitDirichlet()\n      dirichlet_density &lt;- function(a, b, c) {\n          compositions::dDirichlet(cbind(a, b, c), alpha = dirichlet_params$alpha, ...)\n      }\n      res &lt;- Ternary::TernaryPointValues(dirichlet_density, resolution = n)\n      res[\"z\", ]\n    } else {\n      0\n    }\n  })\n\n  dumy_grid[\"z\", ] &lt;- Reduce(\"+\", values)\n  dumy_grid\n}\n\n\nNow we apply the kernel density function to our data:\n\nACF_kde &lt;- kde_Dirichlet(Aar_ACF, h = 0.25, n = 75)\n\nTernaryPlot(\n  main = \"ACF\",\n  atip = \"A\", btip = \"F\", ctip = \"C\",\n  alab = expression(\"Al\"[2] * \"O\"[3] + \"Fe\"[2] * \"O\"[3] - \"(Na\"[2] * \"O\" + \"K\"[2] * \"O)\"),\n  clab = expression(\"CaO\" + 10 / 3 * \"(P\"[2] * \"O\"[5] * \")\"),\n  blab = expression(\"FeO\" + \"MgO\" + \"MnO\"),\n  point = \"up\",\n  lab.font = 2, tip.font = 2,\n  axis.font = 1,\n  clockwise = FALSE, grid.lines = 5, grid.minor.lines = 1,\n  panel.first = ColourTernary(ACF_kde, spectrum = viridis::magma(512))\n)"
  },
  {
    "objectID": "blog/compositions/index.html#references",
    "href": "blog/compositions/index.html#references",
    "title": "Statistics and composition data",
    "section": "References",
    "text": "References\n\nhttps://www.andrewheiss.com/blog/2023/09/18/understanding-dirichlet-beta-intuition/"
  },
  {
    "objectID": "blog/theriak-domino/index.html",
    "href": "blog/theriak-domino/index.html",
    "title": "Quick-Guide to THERIAK-DOMINO",
    "section": "",
    "text": "This blog post gives a brief and most simple walk-through for THERIAK-DOMINO to generate some phase equilibria diagrams (pseudosection) modeled from the composition of metamorphic rocks."
  },
  {
    "objectID": "blog/theriak-domino/index.html#a-calculate-phases-isopleths",
    "href": "blog/theriak-domino/index.html#a-calculate-phases-isopleths",
    "title": "Quick-Guide to THERIAK-DOMINO",
    "section": "A: Calculate phases / isopleths",
    "text": "A: Calculate phases / isopleths\n\nWe need to define the chemical composition within a preferred chemical system relevant for the metamorphic reactions. In the therin.txt file, you need to add these compositions following the following structure to the end of the file:\n\n500     4000 \n0 SI(111.8429)AL(31.9729)FE(7.7807)MG(6.1284)MN(0.0846)CA(2.8175)NA(11.4878)K(6.3272)TI(0.6887)H(80)O(?) * HT-9 MnNCKFMASHT\nHere, the first line gives the expected temperature (in °C) and pressure (kbar) for the modeling, and the second line specifies the concentrations of the elements relevant for our metamorphism.\nTo generate this line from your geochemical data, you may use my Bulk to therin app\n\nGo to Bulk_to_therin and upload your major element composition (wt. %).\nSelect your desired chemical system and other optional parameters.\nCopy the result under THERIN (mol) and paste it at the end of your therin.txt file.\n\n\nNext run start.bat file in the Working subfolder of your Theriak-Domino directory. Then type\n\ndomino\n\ndata base definition: Select database for the modelling (e.g. tcds55_p07.txt)\ndefinition of X-axis: Define X-axis and its range (TC for Temperature (in ° Celsius), and then two numbers indicating the minimum and maximum temperature):\n\nTC 300 800\n\ndefinition of X-axis: Define Y-axis and its range (P for pressure followed by the minimum maximum pressure in kbar):\n\nP 1000 7000\n\ndefinition of calculation type: Type . for equilibrium phase diagram (eq) or jump to part B for mineral composition isopleth\nlabeling of reactions, precision and smoothness: type 1 for assemblages\n\nCALCULATION STARTS"
  },
  {
    "objectID": "blog/theriak-domino/index.html#b-optional-calculate-phases-and-isopleths",
    "href": "blog/theriak-domino/index.html#b-optional-calculate-phases-and-isopleths",
    "title": "Quick-Guide to THERIAK-DOMINO",
    "section": "B: (Optional): Calculate phases AND isopleths",
    "text": "B: (Optional): Calculate phases AND isopleths\n\nFor eq and isopleths of phase compositions (e.g. garnet), follow steps up to 3. and type: script\nNew JOB filename: domjob.bat\nscript name for eq diagram: script_001_eq\nFollow steps described in A3 - A7\nAlmandine file name script_002_alm\nFollow steps described in B4 - B8. Define the calculation type. The input must follow this structure: phase key nr min max step.\n\n\nphase must be a mineral phase from the database (GT07W2 for garnet data in the database tcds55_p07.txt)\nnr is 1\nkey is an end-member of that mineral phase (e.g. alm for almandine, gr for grossular, py pyrope, spss spessartine) as defined in the database file for that phase\nmin, max and step specify the range and the step-size for which isopleths the compositions should be calculated for (0 1 0.02` means from 0 to 100% in 2% steps.)\n\nGT07W2 alm 1 0 1 0.02\n\nTo label the reactions, type: 1\nRepeat steps 3-7 for additional garnet phases.\n\nFor example:\nGT07W2 alm 1 0.5 0.8 0.01 \nGT07W2 py 1 0.01 0.05 0.01 \nGT07W2 gr 1 0.2 0.5 0.01 \nGT07W2 spss 1 0.05 0.3 0.01\n\nFinal database definition\n\nend\n\nTo start the scripts type and run domjob"
  },
  {
    "objectID": "blog/theriak-domino/index.html#c-export-mineral-reactions-as-table-and-plot",
    "href": "blog/theriak-domino/index.html#c-export-mineral-reactions-as-table-and-plot",
    "title": "Quick-Guide to THERIAK-DOMINO",
    "section": "C: Export mineral reactions as table and plot",
    "text": "C: Export mineral reactions as table and plot\nNext, we have to clean up the labels of the reactions lines and then plot a phase diagram. Therefore, we run the two routines GUZZLER and EXPLOT (GUZZLER: cleans the labels, while EXPLOT, creates the plot and saves it in the plot.ps file).\nIn the command shell type and run the following functions in this sequence:\n\nguzzler\n\nEnter input name. Usually you can just hit ENTER here, unless your specified a script. In that case you may check all the different *.plt file in your working directory and repeat all this steps for each file.\nSpecify the extent of the X-axis (temperature): min and max values (can be the ones specified earlier in domino, followed by a width value, e.g. 15)\nSpecify the extent of the Y-axis (pressure) min and max values (can be the ones specified earlier in domino, followed by a height value, e.g. 15)\nSpecify the size of the labels: 0.2\nSpecify if you want to label the plot: 3 (means YES)\n\nexplot\n\nEnter output name: Same as above, but now for all *.cln files in your working directory.\n\n\nNow make a copy of the generated .ps or .svg file and repeat the same procedure, but this time we create a plot without labels (so you have a look-up file and a clean plot for further editing).\n\nType and execute guzzler: Specify if you want to label the plot: 0 (means No)\nType and execute explot\n\nFinally generated the phase diagrams as a postscript file (*.ps) and *.svg, which you can further edit with vector graphics software, such as the free inkscape program."
  },
  {
    "objectID": "blog/theriak-domino/index.html#d-quickly-search-the-reaction-table-for-mineral-phases",
    "href": "blog/theriak-domino/index.html#d-quickly-search-the-reaction-table-for-mineral-phases",
    "title": "Quick-Guide to THERIAK-DOMINO",
    "section": "D: Quickly search the reaction table for mineral phases",
    "text": "D: Quickly search the reaction table for mineral phases\nIf you open the postscript file, you will see a complicated equilibrium phase diagram with some reactions numbered instead of labeled (because there is not enough room for labels.) To see a list of reactions and numbers, open the file table (created by GUZZLER) in the Working folder.\nTo quickly find the reaction belonging to a number, you may use my GUZZLER Filter app:\n\nGo to https://tobiste.shinyapps.io/Bulk_to_therin/\nOpen the GUZZLER Filter tab\nUpload your reaction table (table)\nType in the mineral of interest to see in which reaction lines the mineral is involved."
  },
  {
    "objectID": "blog/theriak-domino/index.html#more-help",
    "href": "blog/theriak-domino/index.html#more-help",
    "title": "Quick-Guide to THERIAK-DOMINO",
    "section": "More help",
    "text": "More help\n\nA more detailed quick-guide: https://cdn.serc.carleton.edu/files/research_education/equilibria/theriak-domino_getting_started.v3.pdf\nOfficial manual: https://earth.geology.yale.edu/~ajs/SupplementaryData/2016/theriakd/Documentation/TheriakDominoGuide.pdf\nCreate pixel-maps: https://muse.union.edu/hollochk/kurt-hollocher/some-theriak-domino-tips-and-tools/"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Cleaning\n\n\n\nThis project involves using machine learning algorithms to predict house prices based on various features such as location, size, and amenities. It includes data cleaning, feature engineering, and model selection.\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments.\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs.\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Below is a smattering of research areas I have worked in or am currently working in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing stress and strain fields\n\n\n\n\n\n\nPlate motion\n\n\nStress\n\n\n\nThe spherical shape of the Earth curves straight orientations on the Earth’s surface. Thus, a statistical analysis of large tectonic fields, such as stress and strain, is challenging as long as there is no reference system. In this project, we analyse these fields with respect to the first-order source, plate boundary forces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlate motion\n\n\n\n\n\n\nPlate motion\n\n\nPangea\n\n\n\nEvery motion on the earth surface can be described as a rotation around an axis piercing through the Earth’s center. Thus, also plate motion is mathematically described as the rotation of spherical polygons around rotational axes (so called Euler poles or Poles of Rotation). But how do we describe plate motion if these poles migrate as well?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPre-orogenic detrital zircon record of Peri-Gondwana\n\n\n\n\n\n\nPangea\n\n\nVariscan\n\n\nReconstruction\n\n\nMineral deposits\n\n\n\nIdentifying the source end-members for shelf sedimentary rocks incorporated in orogens. This helps to constrain the paleogeography of Peri-Gondwana prior to the assemblage of Pangea.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReconstructing plate motion: From Pannotia to Pangea\n\n\n\n\n\n\nPlate motion\n\n\nPangea\n\n\nVariscan\n\n\nReconstruction\n\n\nMineral deposits\n\n\nSupercontinent cycle\n\n\n\nThe supercontinent cycle is a fundamental concept in geology that describes the periodic assembly and breakup of supercontinents over hundreds of millions of years. By unraveling the break-up history of Pannotia an the assemblage of Pangea, we can gain insights into the mechanisms that drive this cycle, the timescales involved, and the consequences for Earth’s systems.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/reconstruction/index.html",
    "href": "research/reconstruction/index.html",
    "title": "Reconstructing plate motion: From Pannotia to Pangea",
    "section": "",
    "text": "The supercontinent cycle is a fundamental concept in geology that describes the periodic assembly and breakup of supercontinents over hundreds of millions of years. By unraveling the break-up history of Pannotia an the assemblage of Pangea, we can gain insights into the mechanisms that drive this cycle, the timescales involved, and the consequences for Earth’s systems.\n\n\nAudio\nAI-generated[^1] audio summary\n\n\n\n\n\nReconstruction of the supercontinent cycle Pannotia-Pangea (absolute plate motion reference frame). From Kroner et al. (2020).\n\n\nUsing various methods, including paleomagnetic data, geochronology, and plate kinematic modeling, we can reconstruct the past positions and movements of continents. This research serves as a test of existing plate tectonic models and helps to refine our understanding of how plates interact over time. By identifying inconsistencies and discrepancies in these models, such as the conflicting views on Gondwana-Laurussia interaction during the Devonian, we can develop more accurate and comprehensive representations of Earth’s tectonic history.\nPaleozoic orogenies, such as Caledonian, Taconic, Famatinian, Acadian, and Variscan orogenies occurred at different locations and different time. But they do not represent isolated tectonic events - instead they are all related through the Paleozoic motion between Laussia and Gondwana culminating in the assemblage of Pangea. Thus, plate tectonic models and reconstructions must be in agreement with the conclusions we draw from studying these ancient orogens.\nIn this project, we examine Paleozoic plate tectonics, focusing on the formation of the supercontinent Pangaea. We utilize a novel method to reconstruct relative plate movements from continental deformation patterns, specifically analyzing the interactions between Gondwana and Laurussia. Our research integrates various geological data, including paleogeographic, paleobiogeographic, and geochemical information to explain the complex tectonic evolution of western Pangaea and the Central European Variscides.\n\n\n\nPannotia reconstruction. From Kroner et al. (2020).\n\n\n\n\n\n\nReferences\n\nKroner, U., Stephan, T., Romer, R. L., & Roscher, M. (2020). Paleozoic plate kinematics during the Pannotia–Pangaea supercontinent cycle. Geological Society, London, Special Publications, 503(1), 83–104. https://doi.org/10.1144/sp503-2020-15\n\n\nKroner, U., Romer, R. L., & Stephan, T. (2023). Die Rekonstruktion von relativen Plattenbewegungen aus dem paläozoischen Deformationsmuster der Kontinente. [Relative plate motions deduced from the Palaeozoic deformation pattern of continents.]. Zeitschrift Der Deutschen Gesellschaft für Geowissenschaften, 174(3), 491–519. https://doi.org/10.1127/zdgg/2023/0365\n\n\nStephan, T., Kroner, U., Romer, R. L., & Rösel, D. (2019). From a bipartite Gondwanan shelf to an arcuate Variscan belt: The early Paleozoic evolution of northern Peri-Gondwana. Earth-Science Reviews, 192, 491–512. https://doi.org/10.1016/j.earscirev.2019.03.012"
  },
  {
    "objectID": "research/zircon/index.html",
    "href": "research/zircon/index.html",
    "title": "Pre-orogenic detrital zircon record of Peri-Gondwana",
    "section": "",
    "text": "The use of modern approaches to analyze detrital zircon provenance data challenges existing paleogeographic models and refines our understanding of the tectonic history of ancient continental blocks. In particular, detrital zircon provenance analysis may contributes significantly to our understanding of the pre-orogenic paleogeography, the existence and extent of ancient oceanic lithosphere, the operation of large-scale sediment dispersal systems, and the processes of crustal growth and evolution in different parts of the world.\nWe employ statistical approach to analyze the detrital zircon age spectra to understand the provenance of sedimentary rocks and reconstruct paleogeography. Our study focuses on the Peri-Gondwanan crust north of Africa and adjacent areas incorporated in the Variscan orogen."
  },
  {
    "objectID": "research/zircon/index.html#database",
    "href": "research/zircon/index.html#database",
    "title": "Pre-orogenic detrital zircon record of Peri-Gondwana",
    "section": "Database",
    "text": "Database\nWe therefore compiled ca. 60.000 detrital zircons U-Pb ages from 770 Precambrian to Lower Paleozoic sedimentary rocks samples sourced from 160 publications. By analyzing such a vast compilation of zircon ages, we minimized potential biases arising from incomplete or limited datasets. This approach enhances the reliability of the provenance interpretations and their implications for understanding the tectonic history of the Peri-Gondwanan.\n\n\n\nDetrital zircon spectra of the sources for the sedimentary rocks of the Peri-Gondwanan shelf (Stephan et al., 2018, fig. 3)."
  },
  {
    "objectID": "research/zircon/index.html#method-and-results",
    "href": "research/zircon/index.html#method-and-results",
    "title": "Pre-orogenic detrital zircon record of Peri-Gondwana",
    "section": "Method and results",
    "text": "Method and results\nFor the statistical analyses, we include multidimensional scaling and density-based clustering to identify provenance end-member populations representing distinct source regions.\n\n\n\nStatistical analyzis and clustering (Stephan et al., 2018, fig. 5).\n\n\nBy comparing the detrital zircon age spectra of Peri-Gondwanan samples with those from known source areas, we identified four distinct zircon provinces:\n\nLaurussian Zircon Province: Characterized by Meso- to Paleoproterozoic zircon crystals with major age peaks at 0.95 Ga, 1.05-1.15 Ga, 1.5-1.65 Ga, and 1.8 Ga.\nAvalonian Zircon Province: Predominantly contains Mesoproterozoic zircon populations with peaks at 1.2 Ga, ~1.5 Ga, and 2.1-2.2 Ga, suggesting an Amazonian signature.\nWest African Zircon Province: Marked by the Eburnean (2.1 Ga) peak and a 1.8-2.0 Ga peak, lacking ‘Grenvillian’ (1.0 Ga) zircons. This distribution matches that of sedimentary rocks covering the West African Craton.\nEast African–Arabian Zircon Province: Features a 1.8–2.0 Ga peak, a subordinate Eburnean peak, a modest 2.5–2.7 Ga peak, a significant ~1.0 Ga ‘Grenvillian’ peak, and a ~800 Ma peak, resembling sedimentary rocks from Egypt, Israel, Jordan, and Iran.\n\n\n\n\nSpatial distribution of the pre-Variscan zircon provinces along the Variscan basement of Europe and North America [Stephan et al. (2018); Fig. 6]."
  },
  {
    "objectID": "research/zircon/index.html#major-implications",
    "href": "research/zircon/index.html#major-implications",
    "title": "Pre-orogenic detrital zircon record of Peri-Gondwana",
    "section": "Major implications",
    "text": "Major implications\n\nTesting and Refining Paleogeographic Reconstructions: By identifying the source regions of sediments, the analysis allows for a reassessment of the paleogeographic positions of continental blocks. For instance, our study reveals that Iberia, previously believed to have a West African provenance, likely originated from an East African-Arabian source.\nIdentifying Ancient Oceanic Lithosphere: The absence of specific zircon age signatures can be used to infer the presence or absence of oceanic lithosphere between continental blocks. Our study found no evidence of additional oceanic lithosphere besides the Rheic Ocean, suggesting a vast and contiguous Peri-Gondwanan shelf.\nReconstructing Sediment Dispersal Patterns and Super-Fan Systems: Analyzing detrital zircon provenance helps to understand the pathways of ancient sediment transport and the existence of large-scale sediment dispersal systems. The identification of distinct zircon provinces led to the proposal of two independent super-fan systems, the West and East Gondwana super-fan systems, which supplied sediments to the northern Peri-Gondwanan shelf.\nUnderstanding Crustal Growth and Evolution: The distribution of zircon ages provides information about the timing and nature of magmatic and metamorphic events that contributed to the formation of continental crust. Our findings on the age distributions within each zircon province shed light on the crustal evolution of different regions within Peri-Gondwana.\nCorrelating Litho-Biostratigraphic Units and Terranes: Detrital zircon provenance analysis can aid in correlating rock units and terranes across vast distances, even when they are now separated by tectonic processes. Our study’s identification of similar zircon age patterns in geographically disparate regions supports the correlation of certain terranes and lithostratigraphic units."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Lead Instructor\n\nGeological Case Studies (Lakehead University, GEOL-4313)\nStructural Geology and Tectonics (Lakehead University, GEOL 3310)\nMetamorphic Petrology (Lakehead University, GEOL 3217)\n\n\n\nTeaching assistants\n\n3D Modeling (Technische Universität Bergakademie Freiberg, 2018)\n\n\n\nShort-courses and workshops\n\nPlate motion and deformation of the lithosphere (workshop in 2023/10 at Department of Geology, Technische Universität Bergakademie Freiberg, Germany)\nProgramming with R — A Beginners’ Guide for Geoscientists (short course in 2022/09 at University of Calgary, Department of Geoscience, Calgary, AB, Canada)"
  }
]